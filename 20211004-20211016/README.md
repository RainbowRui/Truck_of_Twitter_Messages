### 意见性分享


***
### 课程和报告分享

#### 3DV线上讨论会

NEW APPROACHES TO 3D VISION 1st-4th November 2021 

Pleased to announce this Royal Society (royalsociety) online meeting on 3D vision bringing together researchers in computer vision, animal vision, and human vision.  

Website: [https://royalsociety.org/science-events-and-lectures/2021/11/3d-vision/…](https://t.co/s9OfSMHtgT?amp=1) 

DAY 1 (1st Nov) – Seeing Beyond SLAM  Chair: Andrew Fitzgibbon ([@Awfidius](https://twitter.com/Awfidius))

Session 1: Neural Scene Representation Ali Eslami ([@arkitus](https://twitter.com/arkitus)) + Ida Momennejad ([@criticalneuro](https://twitter.com/criticalneuro)) 

Session 2: Perception-Action Loop Sergey Levine ([@svlevine](https://twitter.com/svlevine)) + Andrew Glennerster([@ag3dvr](https://twitter.com/ag3dvr))

DAY 2 (2nd Nov) – Animals in Action  Chair: Matteo Carandini ([@MatteoCarandini](https://twitter.com/MatteoCarandini)) 

Session 1: Locating Prey + Reward Jenny Read ([@jcaread](https://twitter.com/jcaread)) + Aman Saleem ([@Aman_sal](https://twitter.com/Aman_sal)) 

Session 2: Navigation in 3D Space Kate Jeffery ([@drkjjeffery](https://twitter.com/drkjjeffery)) + Gily Ginosar ([@gilyginosar](https://twitter.com/gilyginosar))

DAY 3 (3rd Nov) – Experiencing Space Chair: Mar Gonzalez-Franco ([@twi_mar](https://twitter.com/twi_mar)) 

Session 1: Theories of Visual Space Dhanraj Vishwanath + Paul Linton ([@LintonVision](https://twitter.com/LintonVision)) 

Session 2: Challenges for Virtual Reality Sarah Creem-Regehr ([@SarahCreem](https://twitter.com/SarahCreem)) + Douglas Lanman ([@douglaslanman](https://twitter.com/douglaslanman))

DAY 4 (4th Nov) - Panel Discussion  The Chairs, Andrew Fitzgibbon ([@Awfidius](https://twitter.com/Awfidius)), Matteo Carandini ([@MatteoCarandini](https://twitter.com/MatteoCarandini)), Mar Gonzalez-Franco ([@twi_mar](https://twitter.com/twi_mar)), + Jody Culham ([@CulhamARI_Lab](https://twitter.com/CulhamARI_Lab)), share their thoughts on future directions for 3D vision

reference:https://twitter.com/LintonVision/status/1446146817608060928

***
### 成果推荐及讨论
- ##### A Constraint-based Formulation of Stable Neo-Hookean Materials
  [Miles Macklin](https://twitter.com/milesmacklin)

  Our latest paper shows how to add Neo-Hookean FEM to PBD using two simple constraint functions. 
  Paper: https://mmacklin.com/neohookean.pdf, 
  Video: https://mmacklin.com/neohookean_mig.mp4, 
  Demo: https://mmacklin.com/neohookean.html

  reference:https://twitter.com/milesmacklin/status/1445853253145559047
  
- ##### ResNet strikes back: An improved training procedure in timm 
  [Ross Wightman](https://twitter.com/wightmanr)

  pdf: [https://arxiv.org/pdf/2110.00476.pdf…](https://t.co/PDtD3ViJ0W?amp=1) 

  abs: [https://arxiv.org/abs/2110.00476](https://t.co/YQe70YBhZn?amp=1) 

  with more demanding training setting, a vanilla ResNet-50 reaches 80.4% top-1 accuracy at resolution 224×224 on ImageNet-val without extra data or distillation

  <div align=center><img src="https://pbs.twimg.com/media/FA0ZoT6WEAQSp9s?format=jpg&name=medium" alt="Cover" width="75%"/></div>

  - [Leo Boytsov](https://twitter.com/srchvrs):This seems to be a 4 pt increase from their previous result, but why not to explore what's possible with extra data & distillation? Could it be that resnet-50 capacity isn't big enough?
  - [Hervé Jegou](https://twitter.com/hjegou):I would expect distillation to improve the results as well.  But in this ResNet50 paper, we wanted to establish a baseline that couldn't be ignored.  (In DeiT, many subsequent papers did not report our performance with distillation because the considered it a different setting).


reference:https://twitter.com/ak92501/status/1444839293390934021

- ##### Neural implicit humans

  [Michael Black](https://twitter.com/Michael_J_Black)
  
  Neural implicit humans are coming. The key: define a forward skinning deformation field in a pose-independent space. SNARF learns this without direct supervision. At #ICCV2021. Code online.
  
  Video: https://ait.ethz.ch/projects/2021/snarf/downloads/sample2.mp4
  
  - **Gerard Pons-Moll:** Congrats! nice generalization! We also have a neural implicit human paper at ICCV (Neural-GIF). 

  reference: https://twitter.com/Michael_J_Black/status/1444904457007992834?s=20
  
- ##### Indoor Inverse Rendering

  [Zian Wang](https://twitter.com/zianwang97)
  
  Excited to share our #ICCV2021 paper “Learning Indoor Inverse Rendering with 3D Spatially-Varying Lighting”! It handles 3D lighting, disentangles spatially-varying effects, and benefits AR applications.
  
  project page: https://nv-tlabs.github.io/inverse-rendering-3d-lighting/
  
  reference: https://twitter.com/zianwang97/status/1445226424324038657?s=20
