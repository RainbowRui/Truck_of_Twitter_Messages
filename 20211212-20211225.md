### 意见性分享

#### How to store 3D scenes/grids efficiently? 

  [Matthias Niessner](https://twitter.com/MattNiessner)

(1/2)
Just hash occupied voxels, 𝐻(𝑥,𝑦,𝑧)=(𝑥⋅𝑝_1⨁ 𝑦⋅𝑝_2⨁ 𝑧⋅𝑝_3) mod 𝑛, and store them in a linear array!

The voxel hashing method proposed this for 3D recon in 2013, but it's used in many apps such as sparse conv nets!

<div align=center><img src="https://pbs.twimg.com/media/FGl0WSmWUAAWwAL?format=png&name=900x900" alt="Cover" width="60%"/></div>

(2/2)
To access the value of a voxel, simply evaluate the hash, e.g., for a filter kernel or to update a TSDF value.

Why are hashes so powerful? 

Data access *and* insertion is O(1) - ignoring collisions. In comparison, for tree structures, such as octrees, that's O(log(n)).

  [Andreas Kirsch] 
Wasn't that a paper by @sylefeb, too?https://t.co/SUwJlGNxcY

  [Sylvain Lefebvre]
Thanks for the mention! We have some other works on the topic too (e.g. showing that coalesced accesses are possible through a hash on the GPU: https://hal.inria.fr/inria-00624777/document  )

  [game programmer dude]
What do you with dynamic objects moving through the grids? Do you need to remove them from the voxel they were in and then add them to the voxel they move into? Seems like removal of an object from one voxel takes o(n).

  [Morten Vassvik]
I can't find a list of the bounding box spans of all the test cases in the paper. In most cases at least one dimension is missing, e.g. Figure 1 doesn't mention the depth, so I'm not able to compute the efficient sparsity and span of each case. Are these available somewhere? :)

### 课程和报告分享

#### Physical reasoning and inductive biases

Happening tomorrow! #NeurIPS 2021 workshop on physical reasoning and inductive biases for the real world!

Join us at 11 am EST (8 am PST, 4 pm GMT/UTC)

For schedule and accepted papers, see https://physical-reasoning.github.io

Fantastic speakers and panelists from industry/academia

<div align=center><img src="https://pbs.twimg.com/media/FGhZigpXoAQwqVm?format=jpg&name=medium" alt="Cover" width="75%"/></div>

reference: https://twitter.com/_krishna_murthy/status/1470524128922849282?s=20


***

### 成果推荐及讨论

- ##### Hallucinating Pose-Compatible Scenes

  [AK](https://twitter.com/ak92501)
  
  abs: https://arxiv.org/abs/2112.06909
  
  <div align=center><img src="https://pbs.twimg.com/media/FGiVHfcWUAcqRO5?format=jpg&name=medium" alt="Cover" width="60%"/></div>
  
  almost think the non compatible combinations are more promising for art
  
  <div align=center><img src="https://pbs.twimg.com/media/FGlvgIpVUAMrqHl?format=jpg&name=medium" alt="Cover" width="60%"/></div>
  
  reference: https://twitter.com/ak92501/status/1470589245874180096?s=20
  
- ##### ONE MORE POLY

  [EveryPoint](https://twitter.com/EveryPointIO)
  
  We collaborated with Juan Murphy at ONE MORE POLY. His vision: bridging physical + digital content to give creative agencies and brands a new opportunity to create immersive AR and VR content.
  
  reference: https://twitter.com/EveryPointIO/status/1470541263040487424?s=20
  
- #####  repulsive shape optimization

  [Keenan Crane](https://twitter.com/keenanisalive)
  
   Short news story about our work on repulsive shape optimization:https://scs.cmu.edu/news/2021/repulsive-energies

   This is joint work with recent @SCSatCMU rad Chris Yu, Henrik Schumacher from RWTH Aachen University/TU Chemnitz, and high school student Caleb Brakensiek.
   
   homepage:https://www.scs.cmu.edu/news/2021/repulsive-energies

    <div align=center><img src="https://www.scs.cmu.edu/files/images/repulsive-shapes.png" alt="Cover" width="60%"/></div>
    
- #####  Equivariant Subgraph Aggregation Networks
  
  [Hannes Stärk](https://twitter.com/HannesStaerk)
  
  This Tuesday, we'll discuss the paper "Equivariant Subgraph Aggregation Networks" with the authors.
  
  Join us on Zoom at 4pm GMT: https://hannes-stark.com/logag-reading-group
  
  They propose the interesting idea to leverage that we can get more expressive GNNs if we consider perturbations of the original graph in addition to the original graph.
     <div align=center><img src="https://pbs.twimg.com/media/FGf9p0xX0AM3vRk?format=jpg&name=small" alt="Cover" width="60%"/></div>
     
- #####  Self-Calibrating Neural Radiance Fields

  [Andrea Tagliasacchi](https://twitter.com/taiyasaki)

  "Self-Calibrating Neural Radiance Fields"
  https://arxiv.org/abs/2108.13826

  Noteworthy :)
  
     <div align=center><img src="https://pbs.twimg.com/media/FGhw-uPVIAUdgN7?format=png&name=small" alt="Cover" width="60%"/></div>

- ##### Interactive All-Hex Meshing via Cuboid Decomposition

  [Justin Solomon](https://twitter.com/JustinMSolomon)
  
  Today, our brilliant student Lingxiao Li presented "Interactive All-Hex Meshing via Cuboid Decomposition" at @SIGGRAPHAsia. It provides a UI+clever geometric algorithms for interactive polycube-based hex meshing. This is the first time I've been able to hex mesh reliably!
  
  <div align=center><img src="https://pbs.twimg.com/media/FGs3_o5XoAQXk1q?format=jpg&name=4096x4096" alt="Cover" width="60%"/></div>
  
  This was a collaboration between students in our group: Lingxiao+Paul Zhang+@_DimaSmirnov+@MazAbulnaga. I'm so proud they worked together this well; you can see each of their insight in this work.
  
  Download &try (it's fun!): https://github.com/lingxiaoli94/interactive-hex-meshing

  Paper: https://arxiv.org/pdf/2109.06279.pdf
  
  reference: https://twitter.com/JustinMSolomon/status/1471333735270342662?s=20
  
- ##### GPU ray tracing

  [Keenan Crane](https://twitter.com/keenanisalive)
  
  My 1st graphics paper, way back in 2006, was on GPU ray tracing. The idea was to enable fast geometry updates using a "min-max MIP map": http://cs.cmu.edu/~kmcrane/Projects/RayTracingGeometryImages/paper.pdf
  
  @boubek & co revisit this idea for displacement maps, rather than the full geometry. Nice idea—check it out below!
  
  To recap the basic approach: you start by storing your geometry in the very cool "geometry image" format of Gu et al, i.e., an image where the R,G,B components of a pixel describe X,Y,Z locations in space: https://hhoppe.com/proj/gim/. A quad of pixels then describes two triangles.
  
  <div align=center><img src="https://pbs.twimg.com/media/FGzR-vyWQAIzngc?format=jpg&name=large" alt="Cover" width="60%"/></div>
  
  An ordinary MIP maps stores an "image pyramid" where each image is 1/2 the size of the previous one, downsampled by averaging. This "pre-filtering" lets you efficiently approximate integrals of many texels covered by one pixel. Basically all textures today are drawn this way.
  
  <div align=center><img src="https://pbs.twimg.com/media/FGzWn9pXEAEKmlz?format=jpg&name=large" alt="Cover" width="60%"/></div>
  
  As a total aside, the original MIP mapping paper (by Lance Williams in 1983) used this ridiculous frog image in most of the figures.  I'm kind of sad this never become a standard test image in computer graphics.  (Almost looks like something @_AlecJacobson might draw…)
  
  <div align=center><img src="https://pbs.twimg.com/media/FGzW5OWX0AAWn0R?format=jpg&name=large" alt="Cover" width="60%"/></div>
  
  A min-max MIP map does something a bit different: rather than averaging pixels, it takes the minimum (or maximum) in each component, generating a min (or max) image pyramid. Since the RGB values encode XYZ positions, the min/max values give the corners of a bounding box!
  
  So, a min/max pair of image pyramids provides a bounding volume hierarchy (BVH) for the geometry. What's cool about this BVH is that it's *super* fast to compute/update—especially for a GPU in 2006! Just do repeated downsampling, but with a different arithmetic operation.
  
  You can take this idea one step further and precompute, for each texel, where you should go next if your ray hits or misses that bounding box. This way, BVH traversal is "stack free," and can be nicely streamed through the GPU.
  
  As another total aside, I'm reminded @BrianKaris (developer on the UE5 Nanite engine) mused about geometry images in a quest for geometry virtualization: http://graphicrants.blogspot.com/2009/01/virtual-geometry-images.html. In the end Nanite doesn't use 'em—but hey, it at least uses a *min* MIP map for occlusion culling!
  
  Anyway, the point of all this is that by reducing BVH construction to min/max downsampling, it becomes almost a no-op to update the BVH for a deforming scene.  (Check out our weirdo video above, with talking ogres and hopping teapots.)
  
  The @AdobeResearch paper takes this whole idea yet another step further, using a min-max MIP map just for displacements on top of a coarse mesh—they also explain how to do traversal when geometry deforms. But I'll let them do the rest of the talking!
  
  reference: https://twitter.com/keenanisalive/status/1471802366596890624?s=20
  
  
